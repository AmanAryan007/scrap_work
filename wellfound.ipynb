{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQWOs-hcLslf"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def process_html_file(html_file_path):\n",
        "    try:\n",
        "        # Read the HTML content from the file\n",
        "        with open(html_file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            html_content = file.read()\n",
        "\n",
        "        # Parse the HTML content using BeautifulSoup\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "        # Find the <link> tag and extract the 'href' attribute value\n",
        "        link_tag = soup.find('link')\n",
        "        if link_tag:\n",
        "            href_value = link_tag.get('href')\n",
        "\n",
        "        # Extract the website link\n",
        "        website_link = soup.find('button', class_='styles_websiteLink___Rnfc').text.strip()\n",
        "\n",
        "        # Extract social media platforms and their associated links\n",
        "        social_links = []\n",
        "        social_buttons = soup.find_all('button', class_='styles_socialLink__r21Yf')\n",
        "        for button in social_buttons:\n",
        "            platform_name = button.text.strip()\n",
        "            social_links.append((platform_name, button.find('svg')['class']))  # Extract platform name and SVG class\n",
        "\n",
        "        # Find and extract the desired information\n",
        "        dt_tags = soup.find_all('dt')\n",
        "\n",
        "        # Initialize variables to store extracted information\n",
        "        company_size = \"None\"\n",
        "        total_raised = \"None\"\n",
        "        company_type = \"None\"\n",
        "        markets = \"None\"\n",
        "\n",
        "        # Loop through <dt> tags to extract desired information\n",
        "        for dt in dt_tags:\n",
        "            if dt.previous_sibling and dt.previous_sibling.name == 'dd':\n",
        "                sibling_text = dt.previous_sibling.get_text(strip=True)\n",
        "                if sibling_text == 'Company size':\n",
        "                    company_size = dt.get_text(strip=True)\n",
        "                elif sibling_text == 'Total raised':\n",
        "                    total_raised = dt.get_text(strip=True)\n",
        "                elif sibling_text == 'Company type':\n",
        "                    company_type = dt.get_text(strip=True)\n",
        "                elif sibling_text == 'Markets':\n",
        "                    # Extract markets from <a> tags\n",
        "                    market_elements = dt.find_all('a')\n",
        "                    markets = ', '.join([element.get_text(strip=True) for element in market_elements])\n",
        "\n",
        "        # Extract text from content div and clean up the text\n",
        "        content_div = soup.find('div', class_='styles_content__XhI8z')\n",
        "        if content_div:\n",
        "            extracted_text = content_div.get_text(separator=' ', strip=True)  # Replace newlines with space\n",
        "\n",
        "            # Extract specific content from <h1> tag with proper spacing\n",
        "            h1_tag = soup.find('h1', class_='text-xl font-medium text-dark-aaaa antialiased mb-4')\n",
        "            if h1_tag:\n",
        "                h1_text = h1_tag.get_text(strip=True)  # Get text content of <h1> tag\n",
        "                # Replace comment with space to ensure proper spacing between words\n",
        "                h1_text_with_space = ' '.join(h1_text.split('<!-- -->'))\n",
        "                extracted_text = h1_text_with_space + ' ' + extracted_text\n",
        "\n",
        "                # Extract company name from <h1> tag text (after removing 'careers')\n",
        "                company_name = h1_text.replace('careers', '')  # Remove 'careers' from company name\n",
        "\n",
        "        else:\n",
        "            extracted_text = None\n",
        "            company_name = None\n",
        "\n",
        "        # Clean and format extracted text for proper spacing\n",
        "        if extracted_text:\n",
        "            # Add space before every occurrence of 'careers'\n",
        "            cleaned_text = extracted_text.replace('careers', ' careers')\n",
        "            # Remove extra whitespace and normalize spacing\n",
        "            cleaned_text = ' '.join(cleaned_text.split())\n",
        "        else:\n",
        "            cleaned_text = None\n",
        "\n",
        "        # Create a dictionary to store the extracted information\n",
        "        data = {\n",
        "            \"wellfound_url\": href_value,\n",
        "            \"company_url\": website_link,\n",
        "            \"company_name\": company_name,\n",
        "            \"description\": cleaned_text,\n",
        "            \"employee_count\": company_size,\n",
        "            \"Total Raised\": total_raised,\n",
        "            \"Company Type\": company_type,\n",
        "            \"Markets\": markets,\n",
        "\n",
        "              # Add company name to the dictionary\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        # If an exception occurs during processing, print the error and return None\n",
        "        print(f\"Error processing file {html_file_path}: {e}\")\n",
        "        data = None\n",
        "\n",
        "    return data\n",
        "\n",
        "def process_html_files_in_directory(html_directory):\n",
        "    # List all files in the specified directory\n",
        "    html_files = os.listdir(html_directory)\n",
        "\n",
        "    # Filter HTML files based on extension\n",
        "    html_files = [file for file in html_files if file.endswith(\".htm\")]\n",
        "\n",
        "    # Initialize a list to collect data from all HTML files\n",
        "    all_data = []\n",
        "\n",
        "    # Process each HTML file in the directory\n",
        "    for html_file in html_files:\n",
        "        html_file_path = os.path.join(html_directory, html_file)\n",
        "        if os.path.isfile(html_file_path):\n",
        "            # Process the HTML file and extract information\n",
        "            file_data = process_html_file(html_file_path)\n",
        "            if file_data is not None:\n",
        "                all_data.append(file_data)\n",
        "\n",
        "    # Create a DataFrame from the collected data\n",
        "    df = pd.DataFrame(all_data)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Specify the directory path where HTML files are present\n",
        "html_directory = \"/content/Untitled Folder\"\n",
        "\n",
        "# Process all HTML files in the directory and create a DataFrame\n",
        "result_df = process_html_files_in_directory(html_directory)\n",
        "\n",
        "# Display the DataFrame containing extracted information\n",
        "pd.set_option('display.max_colwidth', None)  # Display full text content in DataFrame\n",
        "print(result_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the file path where you want to save the CSV file\n",
        "csv_file_path = \"output.csv\"\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "result_df.to_csv(csv_file_path, index=False)"
      ],
      "metadata": {
        "id": "D12T2_hzMAo0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}