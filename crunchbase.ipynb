{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEdG_C5iCwG4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Specify the path to the folder containing HTML files\n",
        "folder_path = '/content/Untitled Folder'\n",
        "\n",
        "# Function to process each HTML file and extract desired information\n",
        "def process_html_file(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            html_content = file.read()\n",
        "\n",
        "        # Parse the HTML content using BeautifulSoup\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "        # Extract description text\n",
        "        description_element = soup.find('span', class_='description')\n",
        "        about_text = description_element.get_text() if description_element else None\n",
        "\n",
        "        # Extract specific <a> tag text\n",
        "        a_tags = soup.find_all('a', class_='component--field-formatter field-type-enum accent highlight-color-contrast-light ng-star-inserted')\n",
        "        extracted_text = a_tags[0].get_text(strip=True) if a_tags else None\n",
        "\n",
        "        # Extract href attribute from a specific <a> tag\n",
        "        a_tag = soup.find('a', class_='component--field-formatter')\n",
        "        href_value = a_tag['href'] if a_tag else None\n",
        "\n",
        "        # Extract URLs from <a> tags\n",
        "        a_tags = soup.find_all('a', class_='component--field-formatter')\n",
        "        position_to_extract = 3\n",
        "        extracted_url = a_tags[position_to_extract]['href'] if len(a_tags) > position_to_extract else None\n",
        "\n",
        "        # Extract industry values from <chips-container>\n",
        "        chips_container = soup.find('chips-container')\n",
        "        if chips_container:\n",
        "            chip_texts = chips_container.find_all('div', class_='chip-text')\n",
        "            extracted_values = [chip_text.get_text(strip=True) for chip_text in chip_texts]\n",
        "            industry_values_str = ', '.join(extracted_values)\n",
        "        else:\n",
        "            industry_values_str = None\n",
        "         # Extract profile name from a specific element (e.g., <h1>)\n",
        "        profile_name_element = soup.find('h1', class_='profile-name')\n",
        "        profile_name = profile_name_element.get_text() if profile_name_element else None\n",
        "         # Extract founded date from specific <span> element\n",
        "        founded_date_element = soup.find('span', class_='component--field-formatter field-type-date_precision ng-star-inserted')\n",
        "        founded_date = founded_date_element['title'] if founded_date_element else None\n",
        "        return {\n",
        "            'crunchbase_url': href_value,\n",
        "            'Company URL': extracted_url,\n",
        "            'Company Name': profile_name,\n",
        "            'Description': about_text,\n",
        "            'Year Founded': founded_date,\n",
        "            'employee_count': extracted_text,\n",
        "            'Industry': industry_values_str,\n",
        "\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while processing {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# List to store processed data dictionaries\n",
        "data_list = []\n",
        "\n",
        "# Process each HTML file in the folder\n",
        "for file_name in os.listdir(folder_path):\n",
        "    if file_name.endswith('.html') or file_name.endswith('.htm'):\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        extracted_data = process_html_file(file_path)\n",
        "        if extracted_data:\n",
        "            data_list.append(extracted_data)\n",
        "\n",
        "# Create a DataFrame from the processed data\n",
        "df = pd.DataFrame(data_list)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path to save the CSV file\n",
        "csv_file_path = \"/content/crunchbase.csv\"\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv(csv_file_path, index=False)\n",
        "\n",
        "# Display a message indicating successful CSV file creation\n",
        "print(f\"CSV file saved successfully at: {csv_file_path}\")"
      ],
      "metadata": {
        "id": "4cpf1qmKC6Dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the extracted data from the Excel file\n",
        "input_file = '/content/crunchbase.csv'\n",
        "df = pd.read_csv(input_file)\n",
        "\n",
        "# Clean up the description format by removing extra spaces and line breaks\n",
        "df['Description'] = df['Description'].apply(lambda x: ' '.join(x.split()) if isinstance(x, str) else x)\n",
        "\n",
        "# Save the cleaned DataFrame back to the Excel file\n",
        "output_file_cleaned = 'crunchbase_cleaned.xlsx'\n",
        "df.to_excel(output_file_cleaned, index=False)\n",
        "\n",
        "# Display the path to the cleaned output file\n",
        "print(f\"Cleaned data has been saved to '{output_file_cleaned}'.\")\n",
        "\n",
        "# Display the first few rows of the cleaned DataFrame\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "uTBrjA4IFo35"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}